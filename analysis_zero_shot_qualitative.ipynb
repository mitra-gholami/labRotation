{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:42:45.469862Z",
     "start_time": "2024-10-09T07:42:42.426928Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.load_results import *\n",
    "from utils.plot_helpers import *\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('default')\n",
    "import torch\n",
    "from utils.analysis_from_interaction import *\n",
    "from language_analysis_local import TopographicSimilarityConceptLevel, encode_target_concepts_for_topsim\n",
    "import os\n",
    "if not os.path.exists('analysis'):\n",
    "    os.makedirs('analysis')\n",
    "#import plotly.express as px\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:18.649262Z",
     "start_time": "2024-10-09T07:56:18.644438Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def objects_to_concepts(sender_input):\n",
    "    \"\"\"reconstruct concepts from objects in interaction\"\"\"\n",
    "    n_targets = int(sender_input.shape[1]/2)\n",
    "    # get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:19.441215Z",
     "start_time": "2024-10-09T07:56:19.437450Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def retrieve_messages(interaction):\n",
    "    \"\"\"retrieve messages from interaction\"\"\"\n",
    "    messages = interaction.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:20.054421Z",
     "start_time": "2024-10-09T07:56:20.051657Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_symbols(messages):\n",
    "    \"\"\"counts symbols in messages\"\"\"\n",
    "    all_symbols = [symbol for message in messages for symbol in message]\n",
    "    symbol_counts = Counter(all_symbols)\n",
    "    return symbol_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:20.702705Z",
     "start_time": "2024-10-09T07:56:20.699185Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_unique_message_set(messages):\n",
    "    \"\"\"returns unique messages as a set ready for set operations\"\"\"\n",
    "    return set(tuple(message) for message in messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T07:56:21.783204Z",
     "start_time": "2024-10-09T07:56:21.780023Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_unique_concept_set(concepts):\n",
    "    \"\"\"returns unique concepts\"\"\"\n",
    "    concept_tuples = []\n",
    "    for objects, fixed in concepts:\n",
    "        tuple_objects = []\n",
    "        for object in objects:\n",
    "            tuple_objects.append(tuple(object))\n",
    "        tuple_objects = tuple(tuple_objects)\n",
    "        tuple_concept = (tuple_objects, tuple(fixed))\n",
    "        concept_tuples.append(tuple_concept)\n",
    "    tuple(concept_tuples)\n",
    "    unique_concepts = set(concept_tuples)\n",
    "    return unique_concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:47:47.514984Z",
     "start_time": "2024-10-09T08:47:47.511436Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = ['(3,4)', '(3,8)', '(3,16)', '(4,4)', '(4,8)', '(5,4)']\n",
    "n_values = [4, 8, 16, 4, 8, 4]\n",
    "n_attributes = [3, 3, 3, 4, 4, 5]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T08:40:06.257383Z",
     "start_time": "2024-10-09T08:40:06.254886Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = ['(3,4)', '(3,8)']\n",
    "n_values = [4, 8]\n",
    "n_attributes = [3, 3]\n",
    "n_epochs = 300\n",
    "n_datasets = len(datasets)\n",
    "paths = ['results/' + d + '_game_size_10_vsf_3' for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:47:59.777736Z",
     "start_time": "2024-10-09T11:47:59.775770Z"
    }
   },
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "zero_shot = True # whether zero-shot simulations are evaluated\n",
    "zero_shot_test = 'generic' # 'generic' or 'specific'\n",
    "test_interactions = True # whether scores should be calculated on test interactions (only with zero shot)\n",
    "test_as = 'test_sampled_unscaled' # 'test' or 'test_sampled_unscaled' or 'test_unscaled' or 'test_fine' \n",
    "setting = \"\"\n",
    "if context_unaware:\n",
    "    setting = setting + 'context_unaware'\n",
    "else:\n",
    "    setting = setting + 'standard'\n",
    "if zero_shot:\n",
    "    setting = setting + '/zero_shot/' + zero_shot_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine vocab size and message reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:49:45.162832Z",
     "start_time": "2024-10-09T11:48:02.093673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isabella\\AppData\\Local\\Temp\\ipykernel_15524\\2049970464.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  interaction_train = torch.load(path_to_interaction_train)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/(3,4)_game_size_10_vsf_3/standard/zero_shot/generic/0/interactions/train/epoch_300/interaction_gpu0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m path_to_interaction_val \u001b[38;5;241m=\u001b[39m (path_to_run \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteractions/validation/epoch_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(n_epochs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/interaction_gpu0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m path_to_interaction_test \u001b[38;5;241m=\u001b[39m (path_to_run \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteractions/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(test_as) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/epoch_0/interaction_gpu0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m interaction_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_interaction_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m interaction_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path_to_interaction_val)\n\u001b[0;32m     11\u001b[0m interaction_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path_to_interaction_test)\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/(3,4)_game_size_10_vsf_3/standard/zero_shot/generic/0/interactions/train/epoch_300/interaction_gpu0'"
     ]
    }
   ],
   "source": [
    "# go through all datasets\n",
    "for i, d in enumerate(datasets):\n",
    "    print(d)\n",
    "    for run in range(5):\n",
    "        path_to_run = paths[i] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "        path_to_interaction_test = (path_to_run + 'interactions/' + str(test_as) +'/epoch_0/interaction_gpu0')\n",
    "        interaction_train = torch.load(path_to_interaction_train)\n",
    "        interaction_val = torch.load(path_to_interaction_val)\n",
    "        interaction_test = torch.load(path_to_interaction_test)\n",
    "        \n",
    "        concepts_train = objects_to_concepts(interaction_train.sender_input)\n",
    "        concepts_val = objects_to_concepts(interaction_val.sender_input)\n",
    "        concepts_test = objects_to_concepts(interaction_test.sender_input)\n",
    "        \n",
    "        messages_train = retrieve_messages(interaction_train)\n",
    "        messages_val = retrieve_messages(interaction_val)\n",
    "        messages_test = retrieve_messages(interaction_test)\n",
    "    \n",
    "        symbol_counts_train = count_symbols(messages_train)\n",
    "        symbol_counts_val = count_symbols(messages_val)\n",
    "        symbol_counts_test = count_symbols(messages_test)\n",
    "        symbol_counts = [symbol_counts_train, symbol_counts_val, symbol_counts_test]\n",
    "        pickle.dump(symbol_counts, open(path_to_run + 'symbol_counts_' + str(test_as) + '.pkl', 'wb'))\n",
    "        \n",
    "        # consider train and validation messages together\n",
    "        messages_train_val = messages_train +  messages_val\n",
    "        # consider only unique messages\n",
    "        messages_train_val_unique = get_unique_message_set(messages_train_val)\n",
    "        #print(\"messages train val\", len(messages_train_val), len(messages_train_val_unique))\n",
    "        messages_test_unique = get_unique_message_set(messages_test)\n",
    "        #print(\"messages test\", len(messages_test), len(messages_test_unique))\n",
    "        # total messages\n",
    "        messages_total = messages_train_val +  messages_test\n",
    "        messages_total_unique = get_unique_message_set(messages_total)\n",
    "        \n",
    "        # concepts\n",
    "        concepts_train_unique = get_unique_concept_set(concepts_train)\n",
    "        concepts_val_unique = get_unique_concept_set(concepts_val)\n",
    "        concepts_test_unique = get_unique_concept_set(concepts_test)\n",
    "        #print(\"concepts\", len(concepts_test), len(concepts_test_unique))\n",
    "        concepts_total = concepts_train + concepts_val + concepts_test\n",
    "        concepts_total_unique = get_unique_concept_set(concepts_total)\n",
    "        num_of_concepts = [len(concepts_train_unique), len(concepts_val_unique), len(concepts_test_unique), len(concepts_total_unique), len(concepts_total)]\n",
    "        pickle.dump(num_of_concepts, open(path_to_run + 'num_of_concepts_' + str(test_as) + '.pkl', 'wb'))\n",
    "        \n",
    "        # messages reused in testing:\n",
    "        intersection = messages_train_val_unique & messages_test_unique\n",
    "        \n",
    "        # messages only used in training:\n",
    "        difference_train = messages_train_val_unique - messages_test_unique\n",
    "        \n",
    "        # messages only used in testing:\n",
    "        difference_test = messages_test_unique - messages_train_val_unique\n",
    "        print(len(difference_test), \"novel messages used for the\", len(concepts_test_unique), \"novel concepts\")\n",
    "        \n",
    "        message_reuse = [len(intersection), len(difference_train), len(difference_test), len(concepts_test_unique), (len(difference_test)/len(concepts_test_unique)), len(messages_test_unique)]\n",
    "        pickle.dump(message_reuse, open(path_to_run + 'message_reuse_' + str(test_as) + '.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:49:45.175397Z",
     "start_time": "2024-10-09T11:49:45.170428Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "message_reuse_dict = {'intersection': [], 'difference train': [], 'difference test': [], 'concepts test unique': [], 'test ratio': [], 'messages test unique': [],\n",
    "                      'reuse rate': [], 'novelty rate': [], 'total ratio': []}\n",
    "for i, d in enumerate(datasets):\n",
    "    intersection, train_difference, test_difference, test_concepts, test_ratio, test_messages, reuse_rate, novelty_rate, total_ratio = [], [], [], [], [], [], [], [], []\n",
    "    for run in range(5):\n",
    "        path_to_run = paths[i] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "        message_reuse = pickle.load(open(path_to_run + 'message_reuse_' + str(test_as) + '.pkl', 'rb'))\n",
    "        intersection.append(message_reuse[0])\n",
    "        train_difference.append(message_reuse[1])\n",
    "        test_difference.append(message_reuse[2])\n",
    "        test_concepts.append(message_reuse[3])\n",
    "        test_ratio.append(message_reuse[4])\n",
    "        test_messages.append(message_reuse[5])\n",
    "        reuse_rate.append(message_reuse[0]/message_reuse[5])\n",
    "        novelty_rate.append(message_reuse[2]/message_reuse[5])\n",
    "        total_ratio.append(message_reuse[5]/message_reuse[3]) # test_messages / test_concepts (novel unique messages & concepts)\n",
    "    message_reuse_dict['intersection'].append(intersection)\n",
    "    message_reuse_dict['difference train'].append(train_difference)\n",
    "    message_reuse_dict['difference test'].append(test_difference)\n",
    "    message_reuse_dict['concepts test unique'].append(test_concepts)\n",
    "    message_reuse_dict['test ratio'].append(test_ratio)\n",
    "    message_reuse_dict['messages test unique'].append(test_messages)\n",
    "    message_reuse_dict['reuse rate'].append(reuse_rate)\n",
    "    message_reuse_dict['novelty rate'].append(novelty_rate)\n",
    "    message_reuse_dict['total ratio'].append(total_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T11:49:45.197504Z",
     "start_time": "2024-10-09T11:49:45.192818Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (2) does not match length of index (6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(row)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Create DataFrames\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcol_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Convert DataFrames to LaTeX tables\u001b[39;00m\n\u001b[0;32m     35\u001b[0m latex_table \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mto_latex(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, escape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\pandas\\core\\frame.py:859\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    850\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    851\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    852\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    853\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m         dtype,\n\u001b[0;32m    858\u001b[0m     )\n\u001b[1;32m--> 859\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    868\u001b[0m         data,\n\u001b[0;32m    869\u001b[0m         index,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    873\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    874\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     arrays, refs \u001b[38;5;241m=\u001b[39m \u001b[43m_homogenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# _homogenize ensures\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\pandas\\core\\internals\\construction.py:630\u001b[0m, in \u001b[0;36m_homogenize\u001b[1;34m(data, index, dtype)\u001b[0m\n\u001b[0;32m    627\u001b[0m         val \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mfast_multiget(val, oindex\u001b[38;5;241m.\u001b[39m_values, default\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan)\n\u001b[0;32m    629\u001b[0m     val \u001b[38;5;241m=\u001b[39m sanitize_array(val, index, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 630\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m     refs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    633\u001b[0m homogenized\u001b[38;5;241m.\u001b[39mappend(val)\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (2) does not match length of index (6)"
     ]
    }
   ],
   "source": [
    "message_reuse = [message_reuse_dict['concepts test unique'], message_reuse_dict['messages test unique'], message_reuse_dict['total ratio'], message_reuse_dict['reuse rate'], message_reuse_dict['novelty rate']]\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "mess_reuse_array = np.array(message_reuse)\n",
    "\n",
    "# Compute means and standard deviations over the five runs\n",
    "means = np.mean(mess_reuse_array, axis=-1)\n",
    "std_devs = np.std(mess_reuse_array, axis=-1)\n",
    "\n",
    "# Row names and column names\n",
    "row_names = [\"D(3,4)\", \"D(3,8)\", \"D(3,16)\", \"D(4,4)\", \"D(4,8)\", \"D(5,4)\"]\n",
    "col_names = [\"novel concepts\", \"total unique messages\", \"message-concept ratio\", \"reuse rate\",\"novelty rate\"]\n",
    "\n",
    "# Prepare the data for the DataFrames\n",
    "data = []\n",
    "\n",
    "# iterate over datasets\n",
    "for i in range(means.shape[1]):\n",
    "    row = []\n",
    "    # iterate over conditions\n",
    "    for j in range(means.shape[0]):\n",
    "        if j > 1:\n",
    "            formatted_value = f\"{means[j, i]:.2f} $\\\\pm$ {std_devs[j, i]:.2f}\"\n",
    "        elif j == 0:\n",
    "            formatted_value = f\"{int(means[j, i])}\"\n",
    "        else:\n",
    "            formatted_value = f\"{means[j, i]:.1f} $\\\\pm$ {std_devs[j, i]:.1f}\"\n",
    "        row.append(formatted_value)\n",
    "    data.append(row)\n",
    "\n",
    "# Create DataFrames\n",
    "df = pd.DataFrame(data, index=row_names, columns=col_names)\n",
    "\n",
    "# Convert DataFrames to LaTeX tables\n",
    "latex_table = df.to_latex(index=True, escape=False)\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Symbol reuse\n",
    "Also in \"to generic\" condition, all symbols are reused during testing, i.e. they all encode relevant information. This is why a qualitative analysis of messages makes more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T06:58:19.776172Z",
     "start_time": "2024-10-07T06:58:19.772518Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def symbol_frequency(interaction, n_attributes, n_values, vocab_size, is_gumbel=True):\n",
    "    messages = interaction.message.argmax(dim=-1) if is_gumbel else interaction.message\n",
    "    messages = messages[:, :-1] # without EOS\n",
    "    sender_input = interaction.sender_input\n",
    "    n_objects = sender_input.shape[1]\n",
    "    n_targets = int(n_objects / 2)\n",
    "    # k_hots = sender_input[:, :-n_attributes]\n",
    "    # objects = k_hot_to_attributes(k_hots, n_values)\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values)\n",
    "    # intentions = sender_input[:, -n_attributes:]  # (0=same, 1=any)\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects)\n",
    "\n",
    "    objects[fixed == 1] = np.nan\n",
    "\n",
    "    objects = objects\n",
    "    messages = messages\n",
    "    favorite_symbol = {}\n",
    "    mutual_information = {}\n",
    "    for att in range(n_attributes):\n",
    "        for val in range(n_values):\n",
    "            object_labels = (objects[:, att] == val).astype(int)\n",
    "            max_MI = 0\n",
    "            for symbol in range(vocab_size):\n",
    "                symbol_indices = np.argwhere(messages == symbol)[0]\n",
    "                symbol_labels = np.zeros(len(messages))\n",
    "                symbol_labels[symbol_indices] = 1\n",
    "                MI = normalized_mutual_info_score(symbol_labels, object_labels)\n",
    "                if MI > max_MI:\n",
    "                    max_MI = MI\n",
    "                    max_symbol = symbol\n",
    "            favorite_symbol[str(att) + str(val)] = max_symbol\n",
    "            mutual_information[str(att) + str(val)] = max_MI\n",
    "\n",
    "    return favorite_symbol, mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:18:55.894107Z",
     "start_time": "2024-10-07T07:18:55.892767Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_unaware = False # whether original or context_unaware simulations are evaluated\n",
    "zero_shot = True # whether zero-shot simulations are evaluated\n",
    "zero_shot_test = 'generic' # 'generic' or 'specific'\n",
    "test_interactions = True # whether scores should be calculated on test interactions (only with zero shot)\n",
    "setting = \"\"\n",
    "if context_unaware:\n",
    "    setting = setting + 'context_unaware'\n",
    "else:\n",
    "    setting = setting + 'standard'\n",
    "if zero_shot:\n",
    "    setting = setting + '/zero_shot/' + zero_shot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T07:18:57.329921Z",
     "start_time": "2024-10-07T07:18:57.186314Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Isabella\\AppData\\Local\\Temp\\ipykernel_15524\\3510479097.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  interaction_train = torch.load(path_to_interaction_train)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/(3,4)_game_size_10_vsf_3/standard/zero_shot/generic/0/interactions/train/epoch_300/interaction_gpu0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m path_to_interaction_val \u001b[38;5;241m=\u001b[39m (path_to_run \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteractions/validation/epoch_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(n_epochs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/interaction_gpu0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m path_to_interaction_test \u001b[38;5;241m=\u001b[39m (path_to_run \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteractions/test/epoch_0/interaction_gpu0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m interaction_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_interaction_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m interaction_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path_to_interaction_val)\n\u001b[0;32m      8\u001b[0m interaction_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path_to_interaction_test)\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Isabella\\miniconda3\\envs\\emergab\\lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/(3,4)_game_size_10_vsf_3/standard/zero_shot/generic/0/interactions/train/epoch_300/interaction_gpu0'"
     ]
    }
   ],
   "source": [
    "for run in range(5):\n",
    "    path_to_run = paths[0] + '/' + str(setting) +'/' + str(run) + '/'\n",
    "    path_to_interaction_train = (path_to_run + 'interactions/train/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "    path_to_interaction_val = (path_to_run + 'interactions/validation/epoch_' + str(n_epochs) + '/interaction_gpu0')\n",
    "    path_to_interaction_test = (path_to_run + 'interactions/test/epoch_0/interaction_gpu0')\n",
    "    interaction_train = torch.load(path_to_interaction_train)\n",
    "    interaction_val = torch.load(path_to_interaction_val)\n",
    "    interaction_test = torch.load(path_to_interaction_test)\n",
    "    \n",
    "    # retrieve \"lexicon\" based on mutual information\n",
    "    # hard-code for D(3,4) for now\n",
    "    favorite_symbol, mutual_information = symbol_frequency(interaction_train, n_attributes=3, n_values=4, vocab_size=13)\n",
    "    print(favorite_symbol)\n",
    "\n",
    "    messages = interaction_test.message.argmax(dim=-1)\n",
    "    messages = [msg.tolist() for msg in messages]\n",
    "    sender_input = interaction_test.sender_input\n",
    "    print(sender_input.shape)\n",
    "    n_targets = int(sender_input.shape[1]/2)\n",
    "    # get target objects and fixed vectors to re-construct concepts\n",
    "    target_objects = sender_input[:, :n_targets]\n",
    "    target_objects = k_hot_to_attributes(target_objects, n_values[i])\n",
    "    # concepts are defined by a list of target objects (here one sampled target object) and a fixed vector\n",
    "    (objects, fixed) = retrieve_concepts_sampling(target_objects, all_targets=True)\n",
    "    concepts = list(zip(objects, fixed))\n",
    "\n",
    "    # get distractor objects to re-construct context conditions\n",
    "    distractor_objects = sender_input[:, n_targets:]\n",
    "    distractor_objects = k_hot_to_attributes(distractor_objects, n_values[i])\n",
    "    context_conds = retrieve_context_condition(objects, fixed, distractor_objects)\n",
    "\n",
    "    # get random qualitative samples\n",
    "    #fixed_index = random.randint(0, n_attributes[i]-1) # define a fixed index for the concept\n",
    "    #n_fixed = random.randint(1, n_attributes[i]) # how many fixed attributes?\n",
    "    n_fixed = 3\n",
    "    #fixed_indices = random.sample(range(0, n_attributes[i]), k=n_fixed) # select which attributes are fixed\n",
    "    fixed_indices = [0, 1, 2]\n",
    "    #fixed_value = random.randint(0, n_values[i]-1) # define a fixed value for this index\n",
    "    fixed_values = random.choices(range(0, n_values[i]), k=n_fixed)\n",
    "    fixed_values = [3, 0, 1]\n",
    "    print(n_fixed, fixed_indices, fixed_values)\n",
    "    #index_threshold = 20000 # optional: define some index threshold to make sure that examples are not taken from the beginning of training\n",
    "    # TODO: adapt this loop such that multiple indices can be fixed\n",
    "    all_for_this_concept = []\n",
    "    for idx, (t_objects, t_fixed) in enumerate(concepts):\n",
    "        #if sum(t_fixed) == 1 and t_fixed[fixed_index] == 1:# and idx > index_threshold:\n",
    "        if sum(t_fixed) == n_fixed and all(t_fixed[fixed_index] == 1 for fixed_index in fixed_indices):\n",
    "            for t_object in t_objects:\n",
    "                if all(t_object[fixed_index] == fixed_values[j] for j, fixed_index in enumerate(fixed_indices)):\n",
    "                    all_for_this_concept.append((idx, t_object, t_fixed, context_conds[idx], messages[idx]))\n",
    "                    fixed = t_fixed\n",
    "    if len(all_for_this_concept) > 0:\n",
    "        #sample = random.sample(all_for_this_concept, 20)\n",
    "        sample = all_for_this_concept\n",
    "        column_names = ['game_nr', 'object', 'fixed indices', 'context condition', 'message']\n",
    "        df = pd.DataFrame(sample, columns=column_names)\n",
    "        print(df)\n",
    "        #df.to_csv('analysis/quali_' + str(d) + '_' + str(setting) + '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv', index=False)\n",
    "        #print('saved ' + 'analysis/quali_' + str(d) + '_' + str(setting) + '_' + str(sample[0][1]) + ',' + str(fixed) + 'all.csv')\n",
    "    else:\n",
    "        raise ValueError(\"sample for dataset \" + str(d) + \" could not be generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emergab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
